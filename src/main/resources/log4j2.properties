# Configure Spark logging to write UC and SSP diagnostics to a file
# This file will be packaged into the SparkShell JAR

# Root logger - set to WARN to reduce noise
rootLogger.level = warn
rootLogger.appenderRef.stdout.ref = console
rootLogger.appenderRef.file.ref = diagnosticsFile

# Console appender (for general Spark output)
appender.console.type = Console
appender.console.name = console
appender.console.target = SYSTEM_OUT
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# File appender for UC and SSP diagnostics - HARDCODED PATH
appender.diagnosticsFile.type = File
appender.diagnosticsFile.name = diagnosticsFile
appender.diagnosticsFile.fileName = /tmp/sparkshell_uc_ssp_diagnostics.log
appender.diagnosticsFile.append = true
appender.diagnosticsFile.layout.type = PatternLayout
appender.diagnosticsFile.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p [%c{1}] %m%n

# Set INFO level for Unity Catalog packages to capture our logInfo statements
logger.uc.name = io.unitycatalog
logger.uc.level = info
logger.uc.additivity = false
logger.uc.appenderRef.file.ref = diagnosticsFile
logger.uc.appenderRef.console.ref = console

# Set INFO level for Delta SSP to capture our logInfo statements
logger.deltaSSP.name = org.apache.spark.sql.delta.serverSidePlanning
logger.deltaSSP.level = info
logger.deltaSSP.additivity = false
logger.deltaSSP.appenderRef.file.ref = diagnosticsFile
logger.deltaSSP.appenderRef.console.ref = console

# Keep Spark's internal logs at WARN
logger.spark.name = org.apache.spark
logger.spark.level = warn

# Silence some noisy loggers
logger.jetty.name = org.sparkproject.jetty
logger.jetty.level = warn
logger.replexprTyper.name = org.apache.spark.repl.SparkIMain$exprTyper
logger.replexprTyper.level = warn
logger.replSparkILoopInterpreter.name = org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
logger.replSparkILoopInterpreter.level = warn
